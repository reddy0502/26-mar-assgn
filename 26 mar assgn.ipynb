{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88fa30d-83ce-46ce-b8ad-d2c8a811e77e",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Simple linear regression involves one independent variable and one dependent variable. The goal is to find a linear relationship between the two, which can be used to make predictions about the dependent variable based on the independent variable. For example, we can use simple linear regression to predict a student's exam score (dependent variable) based on the number of hours they studied (independent variable). In this case, the independent variable is a continuous variable.\n",
    "\n",
    "Multiple linear regression, on the other hand, involves two or more independent variables and one dependent variable. The goal is to find a linear relationship between the dependent variable and all the independent variables together. For example, we can use multiple linear regression to predict a home's selling price (dependent variable) based on its square footage, number of bedrooms, and number of bathrooms (independent variables). In this case, the independent variables are continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84240513-2400-4621-b4ce-8c6ce802098b",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "main assumptions of linear regression:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and the independent variables must be linear. This means that the change in the dependent variable should be proportional to the change in the independent variable. A scatter plot of the data can help us check for linearity.\n",
    "\n",
    "Independence: The observations must be independent of each other. \n",
    "\n",
    "Normality: The errors should be normally distributed with a mean of zero. A histogram or a Q-Q plot of the residuals can help us check for normality.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, we can perform the following steps:\n",
    "\n",
    "Plot the data: A scatter plot of the data can help us check for linearity.\n",
    "\n",
    "Examine the data collection process: Make sure that the observations are independent of each other.\n",
    "\n",
    "Plot the residuals: A plot of the residuals against the predicted values can help us check for homoscedasticity.\n",
    "\n",
    "Check for normality: A histogram or a Q-Q plot of the residuals can help us check for normality.\n",
    "\n",
    "Calculate the correlation matrix: Check for high correlation between the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a09b8-5f08-4e66-ab12-e98b400dbc88",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "The intercept represents the value of the dependent variable when all the independent variables are equal to zero. It gives us a baseline for the dependent variable when none of the independent variables are present. The slope represents the change in the dependent variable for a one-unit increase in the independent variable. It tells us the rate of change in the dependent variable for each unit increase in the independent variable.\n",
    "\n",
    "To interpret the slope and intercept in a linear regression model, we can use the following general formula:\n",
    "\n",
    "Y = intercept + slope*X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de68ec7-1068-4029-8996-ec01e0a56232",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "Gradient descent is a popular optimization algorithm used in machine learning to minimize the cost function or the objective function of a model.\n",
    "The basic idea behind gradient descent is to iteratively adjust the values of the model parameters in the direction of the negative gradient of the cost function. The gradient is a vector that points in the direction of the steepest increase in the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb008dc-220d-4087-9915-1dabe4f95bc2",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "The main difference between simple and multiple linear regression is the number of independent variables included in the model. In simple linear regression, there is only one independent variable, while in multiple linear regression, there can be two or more independent variables.\n",
    "\n",
    "The main advantage of multiple linear regression over simple linear regression is that it allows us to model the effects of multiple independent variables on the dependent variable simultaneously, taking into account their joint effects. This can help us understand the complex relationships between multiple variables and the dependent variable and make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e62b30-2911-4e01-a147-6d76a635da99",
   "metadata": {},
   "source": [
    "6ans:\n",
    "\n",
    "Multicollinearity is a common problem in multiple linear regression that occurs when two or more independent variables in the model are highly correlated with each other. This can make it difficult to estimate the individual effects of each independent variable on the dependent variable, and can lead to unstable and unreliable estimates of the regression coefficients.\n",
    "\n",
    "Multicollinearity can be detected through various methods such as:\n",
    "\n",
    "1.Correlation matrix\n",
    "\n",
    "2.Variance Inflation Factor (VIF)\n",
    "\n",
    "3.Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c68071-2f3c-4464-810c-cd100b91c424",
   "metadata": {},
   "source": [
    "7ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
